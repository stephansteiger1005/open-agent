# OpenWebUI Demo Configuration
# OpenWebUI will be available at http://localhost:3000

# No authentication required for demo
WEBUI_AUTH=false

# Ollama Configuration
# Ollama server is accessible at http://localhost:11434 (host) or http://ollama:11434 (Docker network)
# The llama3 model will be automatically pulled when Ollama starts
OLLAMA_BASE_URL=http://ollama:11434

# MCP Server Configuration
# The MCP server uses the official Model Context Protocol Python library (v1.7.1)
# Server is accessible at http://localhost:8080 (host) or http://mcp-server:8080 (Docker network)
# Use base URL in OpenWebUI (without /sse) - the client automatically appends the correct paths
#
# Available tools:
# 1. get_weather - Returns constant weather data for San Francisco
# 2. get_user_info - Returns constant user profile data
#
# To configure in OpenWebUI:
# - Type: MCP - Streamables HTTP
# - URL: http://mcp-server:8080 (base URL, do not include /sse)
# - Authentication: None
